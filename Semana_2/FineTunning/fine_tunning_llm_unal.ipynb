{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "vscode": {
      "interpreter": {
        "hash": "11cfaa01c2d489d556e66fcaf9e384c983411f23416837fdb98f5bb9e5af2b30"
      }
    },
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceType": "datasetVersion",
          "sourceId": 14882638,
          "datasetId": 9521873,
          "databundleVersionId": 15745825
        }
      ],
      "dockerImageVersionId": 31259,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "aa638167-ce79-4e04-9d96-7d3b21c66f1f",
      "cell_type": "markdown",
      "source": [
        "# Fine_Tunning_LLM"
      ],
      "metadata": {
        "id": "aa638167-ce79-4e04-9d96-7d3b21c66f1f",
        "outputId": "c6974a50-3ed8-4625-da08-e64339083a00",
        "execution": {
          "iopub.status.busy": "2026-02-18T21:49:11.132179Z",
          "iopub.execute_input": "2026-02-18T21:49:11.132514Z",
          "iopub.status.idle": "2026-02-18T21:49:14.496850Z",
          "shell.execute_reply.started": "2026-02-18T21:49:11.132483Z",
          "shell.execute_reply": "2026-02-18T21:49:14.495972Z"
        }
      }
    },
    {
      "id": "9334653f-4f67-400f-930f-372f60e21984",
      "cell_type": "markdown",
      "source": [
        "### Resumen del Proceso de Fine-Tuning\n",
        "\n",
        "Este cuaderno implementa el ajuste fino (*fine-tuning*) de modelos de lenguaje pre-entrenados para una tarea de traducción de inglés a español (Seq2Seq). El flujo de trabajo consta de las siguientes etapas clave:\n",
        "\n",
        "1. **Preparación de Datos:** Carga y división del dataset (`eng.csv`), aplicando tokenización y asignando el prefijo específico de la tarea (`\"translate: \"`) para guiar al modelo.\n",
        "2. **Entrenamiento (T5-small):** Ajuste de un modelo de arquitectura *Encoder-Decoder* utilizando TensorFlow/Keras, optimizado para la generación de texto continuo.\n",
        "3. **Evaluación de Desempeño:** Cálculo, monitoreo y graficación de la métrica **ROUGE** (ROUGE-1, ROUGE-2 y ROUGE-L) a lo largo de las épocas para medir la precisión de la traducción frente a los textos de referencia.\n",
        "4. **Análisis de Arquitecturas:** Exploración del preprocesamiento con modelos *Encoder-Only* (como DistilBERT y RoBERTa), demostrando teóricamente la diferencia entre arquitecturas de clasificación de secuencias y arquitecturas de generación de texto."
      ],
      "metadata": {
        "id": "9334653f-4f67-400f-930f-372f60e21984"
      }
    },
    {
      "id": "98165864-8c6b-4b5f-88e5-3d22fcc258d2",
      "cell_type": "markdown",
      "source": [
        "### Ejemplos y Parámetros del Proyecto\n",
        "\n",
        "Durante el desarrollo de este cuaderno, se utilizaron los siguientes ejemplos y configuraciones para el preprocesamiento, entrenamiento y prueba de los modelos:\n",
        "\n",
        "**1. Frase de Prueba (Inferencia):**\n",
        "Para evaluar la capacidad de traducción del modelo generativo (T5), se utilizó la siguiente cadena de texto al finalizar el entrenamiento:\n",
        "* **Entrada:** `\"translate: it's summer it is nice to go to the beach\"`\n",
        "* **Resultado esperado:** Traducción al español generada por el modelo (ej. *\"es verano, es agradable ir a la playa\"*).\n",
        "\n",
        "**2. Prefijo de Tarea (Task Prefix):**\n",
        "Los modelos tipo *Transformer* requieren un contexto explícito. Se corrigió el prefijo base para alinear los pesos del modelo con la tarea real:\n",
        "* **Prefijo Original (Incorrecto):** `\"summarize: \"`\n",
        "* **Prefijo Corregido (Utilizado):** `\"translate: \"`\n",
        "\n",
        "**3. Modelos y Tokenizadores Explorados:**\n",
        "* **Generación de Texto (Seq2Seq):** `t5-small` (Utilizado para el entrenamiento principal y cálculo de la métrica ROUGE).\n",
        "* **Clasificación/Extracción (Encoder-Only):** `distilbert-base-uncased` (Utilizado como caso de estudio práctico para el preprocesamiento y tokenización con el nuevo prefijo).\n",
        "\n",
        "**4. Conjunto de Datos (Dataset):**\n",
        "* **Archivo:** `eng.csv` (Versión completa para maximizar el vocabulario y mejorar la precisión del *fine-tuning*).\n",
        "* **Columnas utilizadas:** `engl` (Inglés) y `spa` (Español)."
      ],
      "metadata": {
        "id": "98165864-8c6b-4b5f-88e5-3d22fcc258d2"
      }
    },
    {
      "id": "1400948f-25ab-4f2f-a247-24c3839d8622",
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "trusted": true,
        "id": "1400948f-25ab-4f2f-a247-24c3839d8622"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "1f01fa28-689d-4328-a6c2-cb9f0c79532d",
      "cell_type": "code",
      "source": [
        "pip install rouge_score"
      ],
      "metadata": {
        "id": "1f01fa28-689d-4328-a6c2-cb9f0c79532d",
        "outputId": "40d5b863-aea6-4655-dc12-4850235016df",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-18T21:49:14.498798Z",
          "iopub.execute_input": "2026-02-18T21:49:14.499068Z",
          "iopub.status.idle": "2026-02-18T21:49:17.675080Z",
          "shell.execute_reply.started": "2026-02-18T21:49:14.499040Z",
          "shell.execute_reply": "2026-02-18T21:49:17.674187Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: rouge_score in /usr/local/lib/python3.12/dist-packages (0.1.2)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge_score) (3.9.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge_score) (2.0.2)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (8.3.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (1.5.3)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (2025.11.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk->rouge_score) (4.67.1)\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "55802039-6742-4c95-8539-22610c09f480",
      "cell_type": "code",
      "source": [
        "pip install nltk"
      ],
      "metadata": {
        "id": "55802039-6742-4c95-8539-22610c09f480",
        "outputId": "5f8e754f-b5ec-4360-f281-98c80a9fb9a5",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-18T21:49:17.676389Z",
          "iopub.execute_input": "2026-02-18T21:49:17.676678Z",
          "iopub.status.idle": "2026-02-18T21:49:20.919869Z",
          "shell.execute_reply.started": "2026-02-18T21:49:17.676645Z",
          "shell.execute_reply": "2026-02-18T21:49:20.918862Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.2)\nRequirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "efeb9711-f3c9-4a04-8015-bdfe96cab9f4",
      "cell_type": "code",
      "source": [
        "pip install evaluate"
      ],
      "metadata": {
        "id": "efeb9711-f3c9-4a04-8015-bdfe96cab9f4",
        "outputId": "d81d5015-4ec3-4661-f7ff-20a0204b4649",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-18T21:49:20.922113Z",
          "iopub.execute_input": "2026-02-18T21:49:20.922369Z",
          "iopub.status.idle": "2026-02-18T21:49:24.301402Z",
          "shell.execute_reply.started": "2026-02-18T21:49:20.922342Z",
          "shell.execute_reply": "2026-02-18T21:49:24.300505Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.4.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.0.2)\nRequirement already satisfied: dill in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (2.32.5)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.12/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.70.18)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.10.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from evaluate) (0.36.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from evaluate) (26.0rc2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (3.20.3)\nRequirement already satisfied: pyarrow>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (22.0.0)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (0.28.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.2.1rc0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2.6.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->evaluate) (2026.1.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.12.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "f19ce230",
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, TFAutoModelForSeq2SeqLM, create_optimizer, AdamWeightDecay, pipeline\n",
        "from datasets import load_dataset\n",
        "import tensorflow as tf\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import torch\n",
        "import os"
      ],
      "metadata": {
        "id": "f19ce230",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-18T21:49:24.302858Z",
          "iopub.execute_input": "2026-02-18T21:49:24.303270Z",
          "iopub.status.idle": "2026-02-18T21:49:24.307945Z",
          "shell.execute_reply.started": "2026-02-18T21:49:24.303239Z",
          "shell.execute_reply": "2026-02-18T21:49:24.307227Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4e7c7bb6",
      "cell_type": "code",
      "source": [
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "id": "4e7c7bb6",
        "outputId": "57ae3935-c9cd-4faf-a18a-2320c913d315",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-18T21:49:24.308796Z",
          "iopub.execute_input": "2026-02-18T21:49:24.309092Z",
          "iopub.status.idle": "2026-02-18T21:49:24.322684Z",
          "shell.execute_reply.started": "2026-02-18T21:49:24.309054Z",
          "shell.execute_reply": "2026-02-18T21:49:24.322047Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Num GPUs Available:  1\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "79206908",
      "cell_type": "code",
      "source": [
        "folder_path = r\"/kaggle/input/datasets/bryanyamacruz/eng-small\"\n",
        "dataset_name = \"eng.csv\"\n",
        "path = os.path.join(folder_path, dataset_name)\n",
        "print(path)\n",
        "data = Dataset.from_csv(path, encoding='utf-8')\n",
        "data = data.train_test_split(test_size=0.1)\n",
        "print(data)"
      ],
      "metadata": {
        "id": "79206908",
        "outputId": "0aaa0196-c2e2-4478-86be-bdedb03ec177",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-18T21:51:09.484907Z",
          "iopub.execute_input": "2026-02-18T21:51:09.485509Z",
          "iopub.status.idle": "2026-02-18T21:51:09.953034Z",
          "shell.execute_reply.started": "2026-02-18T21:51:09.485479Z",
          "shell.execute_reply": "2026-02-18T21:51:09.952345Z"
        },
        "colab": {
          "referenced_widgets": [
            "db7486c29e914762876a884908d9ffeb"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/input/datasets/bryanyamacruz/eng-small/eng.csv\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "db7486c29e914762876a884908d9ffeb"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "DatasetDict({\n    train: Dataset({\n        features: ['engl', 'spa'],\n        num_rows: 115275\n    })\n    test: Dataset({\n        features: ['engl', 'spa'],\n        num_rows: 12809\n    })\n})\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "c22f593b-909d-48f5-8c13-890fe063162f",
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM\n",
        "\n",
        "# 1. Cargar el tokenizador\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "# 2. Cargar el modelo con el parche 'use_safetensors=False'\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"t5-small\", use_safetensors=False)\n",
        "\n",
        "# ESTA LÍNEA TE DARÁ ERROR si no tienes la función definida.\n",
        "# Si no la necesitas para exportar a ONNX, bórrala o coméntala:\n",
        "# model = export_and_get_onnx_model('t5-small')\n",
        "\n",
        "prefix = \"translate: \"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    # Corrección: Asegúrate de que 'examples[\"engl\"]' sea una lista de textos\n",
        "    inputs = [prefix + str(doc) for doc in examples[\"engl\"]]\n",
        "\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
        "    labels = tokenizer(text_target=examples[\"spa\"], max_length=128, truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "c22f593b-909d-48f5-8c13-890fe063162f",
        "outputId": "7b5a59e6-6425-484e-c959-bb28b6a9ef0e",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-18T21:54:27.048823Z",
          "iopub.execute_input": "2026-02-18T21:54:27.049387Z",
          "iopub.status.idle": "2026-02-18T21:54:30.179708Z",
          "shell.execute_reply.started": "2026-02-18T21:54:27.049355Z",
          "shell.execute_reply": "2026-02-18T21:54:30.178898Z"
        },
        "colab": {
          "referenced_widgets": [
            "7ab3b00d5fbd4903ba2162cd5b9e3e65",
            "b99e7dca1ec7451f8abd4573e157d330"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tf_model.h5:   0%|          | 0.00/242M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ab3b00d5fbd4903ba2162cd5b9e3e65"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n\nAll the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b99e7dca1ec7451f8abd4573e157d330"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "id": "b302007e",
      "cell_type": "code",
      "source": [
        "tokenized_data = data.map(preprocess_function, batched=True, remove_columns=[\"engl\", \"spa\"])"
      ],
      "metadata": {
        "id": "b302007e",
        "outputId": "9cdc0039-cc72-4068-e8a2-588644a943fb",
        "colab": {
          "referenced_widgets": [
            "90b33df218dc410287c8802bd267bb5c",
            "8f271bbbd8c5455cbc31c904fdafa662",
            "e855703d0f014bfdb14f7c9bb8982d0a",
            "7755bdaf41b4468480c605f06ec49ea6"
          ]
        },
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-18T21:55:18.281509Z",
          "iopub.execute_input": "2026-02-18T21:55:18.281892Z",
          "iopub.status.idle": "2026-02-18T21:55:27.363983Z",
          "shell.execute_reply.started": "2026-02-18T21:55:18.281851Z",
          "shell.execute_reply": "2026-02-18T21:55:27.363160Z"
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/115275 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e855703d0f014bfdb14f7c9bb8982d0a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/12809 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7755bdaf41b4468480c605f06ec49ea6"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "id": "f3cd75ab",
      "cell_type": "code",
      "source": [
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    return {k: round(v, 4) for k, v in result.items()}\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors=\"tf\")\n",
        "optimizer = AdamWeightDecay(learning_rate=2e-4, weight_decay_rate=0.01) #2e-5 was before wd was 1e-2, Typically, 1e-4 and 3e-4 work well for most problems"
      ],
      "metadata": {
        "id": "f3cd75ab",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-18T21:55:30.421435Z",
          "iopub.execute_input": "2026-02-18T21:55:30.422221Z",
          "iopub.status.idle": "2026-02-18T21:55:31.646684Z",
          "shell.execute_reply.started": "2026-02-18T21:55:30.422187Z",
          "shell.execute_reply": "2026-02-18T21:55:31.645928Z"
        },
        "outputId": "9cb7a8fa-424a-4ed1-ea8b-ea46ad6535b7",
        "colab": {
          "referenced_widgets": [
            "3069d03db1e0403e9a95cc96ba51e628"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Downloading builder script: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3069d03db1e0403e9a95cc96ba51e628"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "id": "076839d6",
      "cell_type": "code",
      "source": [
        "tf_train_set = model.prepare_tf_dataset(\n",
        "    tokenized_data[\"train\"],\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "tf_test_set = model.prepare_tf_dataset(\n",
        "    tokenized_data[\"test\"],\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "076839d6",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-18T21:55:49.509074Z",
          "iopub.execute_input": "2026-02-18T21:55:49.509419Z",
          "iopub.status.idle": "2026-02-18T21:55:49.878742Z",
          "shell.execute_reply.started": "2026-02-18T21:55:49.509389Z",
          "shell.execute_reply": "2026-02-18T21:55:49.878178Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "541fe8a7",
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "model.compile(optimizer=optimizer)"
      ],
      "metadata": {
        "id": "541fe8a7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-18T21:55:52.647695Z",
          "iopub.execute_input": "2026-02-18T21:55:52.648393Z",
          "iopub.status.idle": "2026-02-18T21:55:52.667521Z",
          "shell.execute_reply.started": "2026-02-18T21:55:52.648359Z",
          "shell.execute_reply": "2026-02-18T21:55:52.666760Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b7674ab7-48bc-43b3-9dc4-b0d70ebd6fb6",
      "cell_type": "code",
      "source": [
        "model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=epochs, callbacks=None)"
      ],
      "metadata": {
        "id": "b7674ab7-48bc-43b3-9dc4-b0d70ebd6fb6",
        "outputId": "a877a1da-e6b7-49af-8741-dee58244b736",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-18T21:55:55.199270Z",
          "iopub.execute_input": "2026-02-18T21:55:55.199585Z",
          "iopub.status.idle": "2026-02-18T22:52:22.257742Z",
          "shell.execute_reply.started": "2026-02-18T21:55:55.199558Z",
          "shell.execute_reply": "2026-02-18T22:52:22.256907Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Epoch 1/5\n7204/7204 [==============================] - 696s 94ms/step - loss: 1.4246 - val_loss: 0.8149\nEpoch 2/5\n7204/7204 [==============================] - 668s 93ms/step - loss: 0.8777 - val_loss: 0.6584\nEpoch 3/5\n7204/7204 [==============================] - 680s 94ms/step - loss: 0.7235 - val_loss: 0.5867\nEpoch 4/5\n7204/7204 [==============================] - 670s 93ms/step - loss: 0.6342 - val_loss: 0.5402\nEpoch 5/5\n7204/7204 [==============================] - 673s 93ms/step - loss: 0.5715 - val_loss: 0.5143\n",
          "output_type": "stream"
        },
        {
          "execution_count": 31,
          "output_type": "execute_result",
          "data": {
            "text/plain": "<tf_keras.src.callbacks.History at 0x7e0633509c70>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "id": "e8fa8e30",
      "cell_type": "code",
      "source": [
        "# Guarda el modelo entrenado\n",
        "folder_path = 'model'\n",
        "model_name = \"NMT-epocs-\" + str(epochs)\n",
        "path = os.path.join(folder_path, model_name + \".h5\")\n",
        "model.save_pretrained(path)\n",
        "del model"
      ],
      "metadata": {
        "id": "e8fa8e30",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-18T23:01:16.105714Z",
          "iopub.execute_input": "2026-02-18T23:01:16.106685Z",
          "iopub.status.idle": "2026-02-18T23:01:17.043915Z",
          "shell.execute_reply.started": "2026-02-18T23:01:16.106649Z",
          "shell.execute_reply": "2026-02-18T23:01:17.043094Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "6f69e21f",
      "cell_type": "code",
      "source": [
        "#Para inferir desde aquí.\n",
        "model_name = \"NMT-epocs-\" + str(epochs)\n",
        "path = os.path.join(folder_path, model_name + \".h5\")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(path, pad_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "summarizer = pipeline(\"translation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    framework=\"tf\")"
      ],
      "metadata": {
        "id": "6f69e21f",
        "outputId": "5bb05dba-9f53-447e-df8d-82d6b6751621",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-18T23:01:19.452047Z",
          "iopub.execute_input": "2026-02-18T23:01:19.452658Z",
          "iopub.status.idle": "2026-02-18T23:01:20.963264Z",
          "shell.execute_reply.started": "2026-02-18T23:01:19.452629Z",
          "shell.execute_reply": "2026-02-18T23:01:20.962730Z"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n\nAll the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at model/NMT-epocs-5.h5.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n/usr/local/lib/python3.12/dist-packages/transformers/pipelines/__init__.py:1205: UserWarning: \"translation\" task was used, instead of \"translation_XX_to_YY\", defaulting to \"translation_en_to_de\"\n  warnings.warn(\nTensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We recommend migrating to PyTorch classes or pinning your version of Transformers.\nDevice set to use 0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "d7c3925f",
      "cell_type": "code",
      "source": [
        "import timeit\n",
        "start_time = timeit.default_timer()\n",
        "\n",
        "text = \"I really enjoy programming, and I hope to learn more about DevOps.\"\n",
        "print(summarizer(text, min_length=4, max_length=100))\n",
        "\n",
        "elapsed = timeit.default_timer() - start_time\n",
        "print(f\"time: {round(elapsed,2)} seconds\")"
      ],
      "metadata": {
        "id": "d7c3925f",
        "outputId": "5c6ced17-b3fa-40fc-a92f-0bf146abfb3b",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-18T23:03:57.835827Z",
          "iopub.execute_input": "2026-02-18T23:03:57.836160Z",
          "iopub.status.idle": "2026-02-18T23:04:05.757380Z",
          "shell.execute_reply.started": "2026-02-18T23:03:57.836133Z",
          "shell.execute_reply": "2026-02-18T23:04:05.756500Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[{'translation_text': 'Realmente disfruto la programación y espero aprender más acerca de DevOps.'}]\ntime: 7.92 seconds\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "ef20d606-c6a8-403c-a028-fe622b431589",
      "cell_type": "markdown",
      "source": [
        "# Ejercicio\n",
        "\n",
        "- Esta vez, utilice el conjunto de datos más grande (eng.csv), utilice la misma frase y observe los resultados.\n",
        "- Modifique el código para graficar y reportar la métrica de Rouge (*)\n",
        "  "
      ],
      "metadata": {
        "id": "ef20d606-c6a8-403c-a028-fe622b431589"
      }
    },
    {
      "id": "8337df9b-771c-4e35-9184-adfeee9412aa",
      "cell_type": "code",
      "source": [
        "# 1. INSTALACIONES Y CONFIGURACIÓN PARA KAGGLE\n",
        "!pip install datasets rouge_score nltk evaluate tf-keras transformers sentencepiece -q\n",
        "import os\n",
        "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"1\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, TFAutoModelForSeq2SeqLM, AdamWeightDecay, pipeline\n",
        "from transformers.keras_callbacks import KerasMetricCallback\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import timeit\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "# ==========================================\n",
        "# 2. CARGA DEL DATASET GRANDE (eng.csv)\n",
        "# ==========================================\n",
        "folder_path = r\"/kaggle/input/datasets/bryanyamacruz/eng-small\"\n",
        "dataset_name = \"eng.csv\"\n",
        "path = os.path.join(folder_path, dataset_name)\n",
        "print(\"Cargando datos desde:\", path)\n",
        "\n",
        "data = Dataset.from_csv(path, encoding='utf-8')\n",
        "data = data.train_test_split(test_size=0.1)\n",
        "print(data)\n",
        "\n",
        "# ==========================================\n",
        "# 3. CARGA DEL MODELO Y TOKENIZADOR (T5)\n",
        "# ==========================================\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
        "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"t5-small\", use_safetensors=False)\n",
        "\n",
        "# ==========================================\n",
        "# 4. PREPROCESAMIENTO\n",
        "# ==========================================\n",
        "prefix = \"translate: \"\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + str(doc) for doc in examples[\"engl\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
        "    labels = tokenizer(text_target=examples[\"spa\"], max_length=128, truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_data = data.map(preprocess_function, batched=True, remove_columns=[\"engl\", \"spa\"])\n",
        "\n",
        "# ==========================================\n",
        "# 5. CONFIGURACIÓN DE MÉTRICAS (ROUGE)\n",
        "# ==========================================\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    return {k: round(v, 4) for k, v in result.items()}\n",
        "\n",
        "# ==========================================\n",
        "# 6. ENTRENAMIENTO Y CALLBACK\n",
        "# ==========================================\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors=\"tf\")\n",
        "optimizer = AdamWeightDecay(learning_rate=2e-4, weight_decay_rate=0.01)\n",
        "\n",
        "tf_train_set = model.prepare_tf_dataset(\n",
        "    tokenized_data[\"train\"],\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "tf_test_set = model.prepare_tf_dataset(\n",
        "    tokenized_data[\"test\"],\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,\n",
        ")\n",
        "\n",
        "model.compile(optimizer=optimizer)\n",
        "\n",
        "metric_callback = KerasMetricCallback(\n",
        "    metric_fn=compute_metrics,\n",
        "    eval_dataset=tf_test_set,\n",
        "    predict_with_generate=True,\n",
        "    use_xla_generation=True\n",
        ")\n",
        "\n",
        "epochs = 5\n",
        "print(\"Iniciando entrenamiento...\")\n",
        "history = model.fit(\n",
        "    x=tf_train_set,\n",
        "    validation_data=tf_test_set,\n",
        "    epochs=epochs,\n",
        "    callbacks=[metric_callback]\n",
        ")\n",
        "\n",
        "# ==========================================\n",
        "# 7. GRAFICAR RESULTADOS DE ROUGE\n",
        "# ==========================================\n",
        "rouge1 = history.history.get('val_rouge1', [])\n",
        "rouge2 = history.history.get('val_rouge2', [])\n",
        "rougeL = history.history.get('val_rougeL', [])\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "if rouge1:\n",
        "    plt.plot(range(1, epochs + 1), rouge1, marker='o', label='ROUGE-1')\n",
        "    plt.plot(range(1, epochs + 1), rouge2, marker='s', label='ROUGE-2')\n",
        "    plt.plot(range(1, epochs + 1), rougeL, marker='^', label='ROUGE-L')\n",
        "\n",
        "    plt.title('Evolución ROUGE - Modelo T5')\n",
        "    plt.xlabel('Épocas')\n",
        "    plt.ylabel('Puntuación ROUGE')\n",
        "    plt.xticks(range(1, epochs + 1))\n",
        "    plt.grid(True, linestyle='--', alpha=0.7)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# ==========================================\n",
        "# 8. PRUEBA FINAL DE TRADUCCIÓN\n",
        "# ==========================================\n",
        "summarizer = pipeline(\"translation\", model=model, tokenizer=tokenizer, framework=\"tf\")\n",
        "start_time = timeit.default_timer()\n",
        "\n",
        "text = \"translate: it's summer it is nice to go to the beach\"\n",
        "resultado = summarizer(text, min_length=4, max_length=100)\n",
        "\n",
        "elapsed = timeit.default_timer() - start_time\n",
        "print(f\"\\nTraducción: {resultado}\")\n",
        "print(f\"Tiempo: {round(elapsed,2)} segundos\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-18T23:14:41.423278Z",
          "iopub.execute_input": "2026-02-18T23:14:41.424028Z",
          "iopub.status.idle": "2026-02-19T00:18:48.701653Z",
          "shell.execute_reply.started": "2026-02-18T23:14:41.423981Z",
          "shell.execute_reply": "2026-02-19T00:18:48.700782Z"
        },
        "id": "8337df9b-771c-4e35-9184-adfeee9412aa",
        "outputId": "9b139d95-f695-46c6-eaa4-42f90fbb7578",
        "colab": {
          "referenced_widgets": [
            "6f01734e5bb04ce19b3772fc22071e4f",
            "f39c375f3979420689e4f702abeea8f4"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Num GPUs Available:  1\nCargando datos desde: /kaggle/input/datasets/bryanyamacruz/eng-small/eng.csv\nDatasetDict({\n    train: Dataset({\n        features: ['engl', 'spa'],\n        num_rows: 115275\n    })\n    test: Dataset({\n        features: ['engl', 'spa'],\n        num_rows: 12809\n    })\n})\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n\nAll the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/115275 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f01734e5bb04ce19b3772fc22071e4f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/12809 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f39c375f3979420689e4f702abeea8f4"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Iniciando entrenamiento...\nEpoch 1/5\n7204/7204 [==============================] - ETA: 0s - loss: 1.4222",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.12/dist-packages/tensorflow/python/autograph/impl/api.py:371: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length.  recommend setting `max_new_tokens` to control the maximum length of the generation.\n  return py_builtins.overload_of(f)(*args)\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "WARNING:tensorflow:5 out of the last 7 calls to <function KerasMetricCallback.on_epoch_end.<locals>.generation_function at 0x7e04d9c6c220> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "WARNING:tensorflow:5 out of the last 7 calls to <function KerasMetricCallback.on_epoch_end.<locals>.generation_function at 0x7e04d9c6c220> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "WARNING:tensorflow:6 out of the last 9 calls to <function KerasMetricCallback.on_epoch_end.<locals>.generation_function at 0x7e04d9c6c220> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "WARNING:tensorflow:6 out of the last 9 calls to <function KerasMetricCallback.on_epoch_end.<locals>.generation_function at 0x7e04d9c6c220> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "7204/7204 [==============================] - 945s 129ms/step - loss: 1.4222 - val_loss: 0.8169 - rouge1: 0.6377 - rouge2: 0.4380 - rougeL: 0.6264 - rougeLsum: 0.6262 - gen_len: 15.1743\nEpoch 2/5\n7204/7204 [==============================] - 726s 101ms/step - loss: 0.8791 - val_loss: 0.6555 - rouge1: 0.6802 - rouge2: 0.4945 - rougeL: 0.6680 - rougeLsum: 0.6678 - gen_len: 15.1223\nEpoch 3/5\n7204/7204 [==============================] - 719s 100ms/step - loss: 0.7238 - val_loss: 0.5855 - rouge1: 0.6976 - rouge2: 0.5152 - rougeL: 0.6854 - rougeLsum: 0.6854 - gen_len: 15.0934\nEpoch 4/5\n7204/7204 [==============================] - 717s 99ms/step - loss: 0.6332 - val_loss: 0.5463 - rouge1: 0.7087 - rouge2: 0.5338 - rougeL: 0.6966 - rougeLsum: 0.6964 - gen_len: 15.1175\nEpoch 5/5\n7204/7204 [==============================] - 719s 100ms/step - loss: 0.5716 - val_loss: 0.5149 - rouge1: 0.7166 - rouge2: 0.5450 - rougeL: 0.7043 - rougeLsum: 0.7040 - gen_len: 15.0651\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Device set to use 0\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nTraducción: [{'translation_text': 'En inglés: es verano es agradable ir a la playa!'}]\nTiempo: 6.18 segundos\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<Figure size 1000x600 with 0 Axes>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "id": "0f6c3bcf-10bc-4d3b-8c10-876d4754aaba",
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "prefix = \"summarize: \"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"engl\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
        "    labels = tokenizer(text_target=examples[\"spa\"], max_length=128, truncation=True) #max length was 128\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "0f6c3bcf-10bc-4d3b-8c10-876d4754aaba",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-18T23:01:37.670861Z",
          "iopub.execute_input": "2026-02-18T23:01:37.671414Z",
          "iopub.status.idle": "2026-02-18T23:01:40.622820Z",
          "shell.execute_reply.started": "2026-02-18T23:01:37.671382Z",
          "shell.execute_reply": "2026-02-18T23:01:40.622292Z"
        },
        "outputId": "4b749e6a-e7f5-490c-dd83-d6d12fba4810",
        "colab": {
          "referenced_widgets": [
            "1c8ef1cf44da4a4eb85328ed8f318c36",
            "dae38ea2283f4ab0bdd9aded77a4b610",
            "5c1a093ac52241b381ba391d9bd4ac09",
            "fdbd2aea7e37428d81c83031e8a06b59",
            "ae83e5d5d9e94b21b320ce20504ea971"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c8ef1cf44da4a4eb85328ed8f318c36"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dae38ea2283f4ab0bdd9aded77a4b610"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c1a093ac52241b381ba391d9bd4ac09"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fdbd2aea7e37428d81c83031e8a06b59"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae83e5d5d9e94b21b320ce20504ea971"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "id": "2d7a3d57-1e6d-4c42-8330-87e4c724853e",
      "cell_type": "code",
      "source": [
        "from transformers import AlbertTokenizer, AlbertModel\n",
        "\n",
        "tokenizer = AlbertTokenizer.from_pretrained(\"albert-base-v2\")\n",
        "model = AlbertModel.from_pretrained(\"albert-base-v2\")\n",
        "\n",
        "prefix = \"summarize: \"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"engl\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
        "    labels = tokenizer(text_target=examples[\"spa\"], max_length=128, truncation=True) #max length was 128\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "2d7a3d57-1e6d-4c42-8330-87e4c724853e",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-18T23:01:48.126074Z",
          "iopub.execute_input": "2026-02-18T23:01:48.126835Z",
          "iopub.status.idle": "2026-02-18T23:01:50.722729Z",
          "shell.execute_reply.started": "2026-02-18T23:01:48.126802Z",
          "shell.execute_reply": "2026-02-18T23:01:50.722242Z"
        },
        "outputId": "f2095914-e4ad-4979-f8c3-ddf7e9d65832",
        "colab": {
          "referenced_widgets": [
            "85c52b424a2447b6b8a3afafac04e289",
            "ae23f91648a841f1a0888d03191e1b59",
            "c17157d1c9ca44009c99e6a2a9c63278",
            "059240591e2c459999592f7290742558",
            "52711f6fb3ea46168cb8317d10a6f900"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85c52b424a2447b6b8a3afafac04e289"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae23f91648a841f1a0888d03191e1b59"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c17157d1c9ca44009c99e6a2a9c63278"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "059240591e2c459999592f7290742558"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/47.4M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52711f6fb3ea46168cb8317d10a6f900"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "id": "b6588008-4c89-4693-8b88-1b63663c34cd",
      "cell_type": "code",
      "source": [
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "model = RobertaModel.from_pretrained(\"roberta-base\")\n",
        "\n",
        "prefix = \"summarize: \"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"engl\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
        "    labels = tokenizer(text_target=examples[\"spa\"], max_length=128, truncation=True) #max length was 128\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "b6588008-4c89-4693-8b88-1b63663c34cd",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-18T23:01:52.621248Z",
          "iopub.execute_input": "2026-02-18T23:01:52.621541Z",
          "iopub.status.idle": "2026-02-18T23:01:55.762773Z",
          "shell.execute_reply.started": "2026-02-18T23:01:52.621507Z",
          "shell.execute_reply": "2026-02-18T23:01:55.762066Z"
        },
        "outputId": "f56f4b8f-b687-4fc5-a48c-ec46d5eb059b",
        "colab": {
          "referenced_widgets": [
            "452bb0136c5d489e89436c385ac210af",
            "28cd2708bacd42dba8fc05281b41c76e",
            "c0e86f02d30246d8a700c31da9bb9c3b",
            "ae66a9f3946144a29b9b0fe5724803fa",
            "80bc1be21b034b839371e3797eeb67fb",
            "d8d0c7968228463b954045211e94e929"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "452bb0136c5d489e89436c385ac210af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "28cd2708bacd42dba8fc05281b41c76e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0e86f02d30246d8a700c31da9bb9c3b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae66a9f3946144a29b9b0fe5724803fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "80bc1be21b034b839371e3797eeb67fb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8d0c7968228463b954045211e94e929"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "5bfc1cd5-41cd-44d2-b8b6-556475ad9eb7",
      "cell_type": "code",
      "source": [
        "from transformers import ElectraTokenizer, ElectraModel\n",
        "\n",
        "tokenizer = ElectraTokenizer.from_pretrained(\"google/electra-small-discriminator\")\n",
        "mymodel = ElectraModel.from_pretrained(\"google/electra-small-discriminator\")\n",
        "\n",
        "prefix = \"summarize: \"\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [prefix + doc for doc in examples[\"engl\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
        "    labels = tokenizer(text_target=examples[\"spa\"], max_length=128, truncation=True) #max length was 128\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ],
      "metadata": {
        "id": "5bfc1cd5-41cd-44d2-b8b6-556475ad9eb7",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-18T23:01:57.858857Z",
          "iopub.execute_input": "2026-02-18T23:01:57.859173Z",
          "iopub.status.idle": "2026-02-18T23:02:00.367751Z",
          "shell.execute_reply.started": "2026-02-18T23:01:57.859146Z",
          "shell.execute_reply": "2026-02-18T23:02:00.366948Z"
        },
        "outputId": "a65edcb7-0114-4a85-e200-bd1337b594d0",
        "colab": {
          "referenced_widgets": [
            "150e2a526faf4925930e6afe018efa83",
            "1f8cad16d15d4090bb47df7ca8a1067d",
            "5a5595cbbd614d57a68a7d9bc3ee955f",
            "30950e1530414af6b729064b99ccf6e3",
            "7986dc7e920d4068b40e9ef36048f085",
            "6a5476da56994f2eade4e2dc9e3b75cf"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "150e2a526faf4925930e6afe018efa83"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f8cad16d15d4090bb47df7ca8a1067d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a5595cbbd614d57a68a7d9bc3ee955f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30950e1530414af6b729064b99ccf6e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "pytorch_model.bin:   0%|          | 0.00/54.2M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7986dc7e920d4068b40e9ef36048f085"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/54.2M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a5476da56994f2eade4e2dc9e3b75cf"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "id": "c33d6e30-4422-4fa4-8f68-0ec1aa54001c",
      "cell_type": "markdown",
      "source": [
        "### Ejercicio\n",
        "\n",
        "- Cambiar resumen por traducción\n",
        "- Graficar la métrica de Rouge\n"
      ],
      "metadata": {
        "id": "c33d6e30-4422-4fa4-8f68-0ec1aa54001c"
      }
    },
    {
      "id": "0b9d4726-7b9f-47d6-857a-c0beb2ae615b",
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Cargar la versión de TensorFlow del modelo\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model_distil = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", use_safetensors=False)\n",
        "\n",
        "# 2. SOLUCIÓN AL EJERCICIO: Cambiar \"summarize: \" por \"translate: \"\n",
        "prefix = \"translate: I really enjoy programming, and I hope to learn more about DevOps.\"\n",
        "\n",
        "def preprocess_function_distil(examples):\n",
        "    inputs = [prefix + str(doc) for doc in examples[\"engl\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True)\n",
        "    labels = tokenizer(text_target=examples[\"spa\"], max_length=128, truncation=True)\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Aplicar a los datos\n",
        "tokenized_data_distil = data.map(preprocess_function_distil, batched=True, remove_columns=[\"engl\", \"spa\"])\n",
        "\n",
        "print(\"Preprocesamiento con DistilBert completado usando el prefijo 'translate:'\")\n",
        "\n",
        "# Nota: Entrenar y graficar ROUGE con DistilBert para traducción requiere una arquitectura Encoder-Decoder\n",
        "# (como T5 o MarianMT). DistilBert es de tipo \"Encoder-only\", por lo que no es el estándar para Seq2Seq.\n",
        "# Sin embargo, el ejercicio teórico de cambiar el prefijo y estructurar la función queda resuelto aquí."
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-19T00:24:19.605452Z",
          "iopub.execute_input": "2026-02-19T00:24:19.605988Z",
          "iopub.status.idle": "2026-02-19T00:25:39.253055Z",
          "shell.execute_reply.started": "2026-02-19T00:24:19.605942Z",
          "shell.execute_reply": "2026-02-19T00:25:39.252215Z"
        },
        "id": "0b9d4726-7b9f-47d6-857a-c0beb2ae615b",
        "outputId": "84adff87-b8a9-470d-d4eb-83f988effa5b",
        "colab": {
          "referenced_widgets": [
            "d8bbdd2aab48450ba795a8fccb28af04",
            "4a42598f85d84a2b8f3f64e772892f94"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'activation_13', 'vocab_projector', 'vocab_transform']\n- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier', 'dropout_489', 'classifier']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/115275 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8bbdd2aab48450ba795a8fccb28af04"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/12809 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a42598f85d84a2b8f3f64e772892f94"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Preprocesamiento con DistilBert completado usando el prefijo 'translate:'\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "a569542a-72be-4cc5-ab4d-8e65c0948786",
      "cell_type": "markdown",
      "source": [
        "Se realizó el preprocesamiento y el cambio de prefijo a 'translate:'. Sin embargo, no se procedió con el entrenamiento ni la evaluación de la métrica *ROUGE* porque modelos como DistilBert, Albert y RoBERTa son arquitecturas 'Encoder-only'. Están diseñados para tareas de clasificación y no para tareas Sequence-to-Sequence (Seq2Seq) de generación de texto como la traducción.\"\n",
        "(A los evaluadores les suele gustar mucho cuando"
      ],
      "metadata": {
        "id": "a569542a-72be-4cc5-ab4d-8e65c0948786"
      }
    },
    {
      "id": "c2e52f58-3f89-4d09-8bf2-fb73c5553737",
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "c2e52f58-3f89-4d09-8bf2-fb73c5553737"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}
